{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Crime Prediction\n",
    "Objective: Predict the likelihood of crimes in districts based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "Data Preparation:\n",
    "\n",
    "Select features like district_name, state_name, and crime categories.\n",
    "Encode categorical features using one-hot encoding or label encoding.\n",
    "Normalize numerical features if required.\n",
    "Model Training:\n",
    "\n",
    "Train a regression model (e.g., Linear Regression or Random Forest).\n",
    "Use Total_Crimes (or a similar column) as the target variable.\n",
    "Evaluation:\n",
    "\n",
    "Evaluate the model using metrics like Mean Absolute Error (MAE) and Root Mean Square Error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('districtwise-cyber-crimes-2017-onwards.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = ['state_name', 'district_name']\n",
    "# Convert all columns from the 4th column onwards to numeric, coercing errors\n",
    "data.iloc[:, 3:] = data.iloc[:, 3:].apply(pd.to_numeric, errors='coerce')\n",
    "data['Total_Crimes'] = data.iloc[:, 3:].sum(axis=1)  # Adjust columns as needed\n",
    "target = 'Total_Crimes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "data_encoded = pd.get_dummies(data[features], drop_first=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_encoded, data[target], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Clustering\n",
    "Objective: Group districts with similar crime patterns.\n",
    "Steps:\n",
    "Data Preparation:\n",
    "\n",
    "Normalize the crime data for fair comparison.\n",
    "Use the selected crime categories as features.\n",
    "Clustering:\n",
    "\n",
    "Apply K-Means or DBSCAN.\n",
    "Determine the optimal number of clusters using the elbow method or silhouette score.\n",
    "Visualization:\n",
    "\n",
    "Use scatter plots or heatmaps to visualize clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'crime_features' is a DataFrame containing your features\n",
    "# Handle missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "crime_features_imputed = imputer.fit_transform(crime_features)\n",
    "\n",
    "# Normalize the crime data\n",
    "scaler = StandardScaler()\n",
    "crime_features_scaled = scaler.fit_transform(crime_features_imputed)\n",
    "\n",
    "# Function to determine the optimal number of clusters using silhouette score\n",
    "def find_optimal_clusters(data, max_clusters=10):\n",
    "    silhouette_scores = []\n",
    "    for n_clusters in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(data)\n",
    "        silhouette_avg = silhouette_score(data, clusters)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f'Number of clusters: {n_clusters}, Silhouette Score: {silhouette_avg:.4f}')\n",
    "    return silhouette_scores\n",
    "\n",
    "# Find optimal number of clusters\n",
    "max_clusters = 10  # You can adjust this value based on your needs\n",
    "silhouette_scores = find_optimal_clusters(crime_features_scaled, max_clusters)\n",
    "\n",
    "# Plotting the Silhouette Scores to visualize the optimal number of clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_clusters + 1), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Scores for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range(2, max_clusters + 1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means clustering with the optimal number of clusters (e.g., based on previous results)\n",
    "optimal_n_clusters = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because range starts from 2\n",
    "kmeans_final = KMeans(n_clusters=optimal_n_clusters, random_state=42)\n",
    "clusters_final = kmeans_final.fit_predict(crime_features_scaled)\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "data['Cluster'] = clusters_final\n",
    "\n",
    "# Visualization using scatter plot (using first two features for simplicity)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(crime_features_scaled[:, 0], crime_features_scaled[:, 1], c=clusters_final, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'Clusters of Districts Based on Crime Patterns (n_clusters={optimal_n_clusters})')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Anomaly Detection\n",
    "Objective: Detect districts with unusual crime spikes.\n",
    "Steps:\n",
    "Calculate Statistics:\n",
    "\n",
    "Compute mean and standard deviation of crimes for each district.\n",
    "Detect Anomalies:\n",
    "\n",
    "Use algorithms like Isolation Forest or Local Outlier Factor.\n",
    "Highlight Results:\n",
    "\n",
    "Present anomalies in tables or maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Ensure 'district_name' is of the same type in both dataframes\n",
    "data['district_name'] = data['district_name'].astype(str)\n",
    "\n",
    "# Compute mean and standard deviation of crimes for each district\n",
    "crime_stats = data.groupby('district_name')['Total_Crimes'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Rename columns to avoid conflicts during merge\n",
    "crime_stats.rename(columns={'mean': 'mean_crimes', 'std': 'std_crimes'}, inplace=True)\n",
    "\n",
    "# Merge the statistics back to the original data\n",
    "data = data.merge(crime_stats, on='district_name', how='left')\n",
    "\n",
    "# Use Isolation Forest to detect anomalies\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "data['anomaly'] = iso_forest.fit_predict(data[['Total_Crimes']])\n",
    "\n",
    "# Highlight anomalies\n",
    "anomalies = data[data['anomaly'] == -1]\n",
    "\n",
    "# Present anomalies in a table\n",
    "print(anomalies[['district_name', 'Total_Crimes', 'mean_crimes', 'std_crimes']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temp Code ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's proceed with A. Crime Prediction first and implement it step by step. After that, we can move on to clustering and anomaly detection. Here's how we'll go about it:\n",
    "\n",
    "Step 1: Crime Prediction\n",
    "We'll train a model to predict Total_Crimes based on district and crime category data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('districtwise-cyber-crimes-2017-onwards.csv')\n",
    "\n",
    "# Data preparation\n",
    "# Convert all columns from the 4th column onwards to numeric, coercing errors\n",
    "data.iloc[:, 3:] = data.iloc[:, 3:].apply(pd.to_numeric, errors='coerce')\n",
    "# Combine columns representing crime counts to calculate total crimes (adjust indices as needed)\n",
    "data['Total_Crimes'] = data.iloc[:, 3:].sum(axis=1)\n",
    "\n",
    "# Select features and target\n",
    "features = ['state_name', 'district_name']  # Update columns based on your data\n",
    "target = 'Total_Crimes'\n",
    "\n",
    "# Encode categorical variables\n",
    "data_encoded = pd.get_dummies(data[features], drop_first=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_encoded, data[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
    "\n",
    "# Optional: Save the model for future use\n",
    "import joblib\n",
    "joblib.dump(model, 'crime_prediction_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('districtwise-cyber-crimes-2017-onwards.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset Preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check column names and types\n",
    "print(\"\\nColumn Names and Data Types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values in the Dataset:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check numeric columns (assumed to be crime counts)\n",
    "print(\"\\nNumeric Columns:\")\n",
    "numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "print(numeric_cols)\n",
    "\n",
    "# Confirm presence of required categorical columns\n",
    "required_columns = ['state_name', 'district_name', 'cyber_crime_category']\n",
    "for col in required_columns:\n",
    "    if col in data.columns:\n",
    "        print(f\"Column '{col}' exists in the dataset.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' is missing!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
